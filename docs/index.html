<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SuperF: Neural Implicit Fields for Multi-Image Super-Resolution</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #fff;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 40px;
        }

        /* Header */
        header {
            padding: 80px 0 60px;
            text-align: center;
            border-bottom: 1px solid #e0e0e0;
        }

        h1 {
            font-size: 48px;
            font-weight: 700;
            color: #1a1a1a;
            margin-bottom: 20px;
            letter-spacing: -0.02em;
        }

        .authors {
            font-size: 18px;
            color: #666;
            margin-bottom: 30px;
        }

        .links {
            display: flex;
            justify-content: center;
            gap: 20px;
            flex-wrap: wrap;
        }

        .link-button {
            display: inline-block;
            padding: 10px 24px;
            background-color: #1a1a1a;
            color: white;
            text-decoration: none;
            border-radius: 4px;
            font-weight: 500;
            transition: background-color 0.3s;
        }

        .link-button:hover {
            background-color: #333;
        }

        .link-button.secondary {
            background-color: #fff;
            color: #1a1a1a;
            border: 1px solid #1a1a1a;
        }

        .link-button.secondary:hover {
            background-color: #f5f5f5;
        }

        /* Hero Image */
        .hero-image {
            margin: 60px 0;
            text-align: center;
        }

        .hero-image img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.1);
        }

        /* Sections */
        section {
            margin: 80px 0;
        }

        h2 {
            font-size: 32px;
            font-weight: 700;
            color: #1a1a1a;
            margin-bottom: 30px;
            letter-spacing: -0.01em;
        }

        h3 {
            font-size: 24px;
            font-weight: 600;
            color: #1a1a1a;
            margin-top: 40px;
            margin-bottom: 20px;
        }

        p {
            font-size: 18px;
            color: #444;
            margin-bottom: 20px;
            line-height: 1.8;
        }

        /* Abstract */
        .abstract {
            background-color: #f8f9fa;
            padding: 40px;
            border-radius: 8px;
            margin: 40px 0;
            border-left: 4px solid #1a1a1a;
        }

        .abstract p {
            margin-bottom: 0;
        }

        /* Method/Approach */
        .method-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 30px;
            margin: 40px 0;
        }

        .method-item {
            padding: 30px;
            background-color: #f8f9fa;
            border-radius: 8px;
        }

        .method-item h4 {
            font-size: 20px;
            font-weight: 600;
            margin-bottom: 15px;
            color: #1a1a1a;
        }

        .method-item p {
            font-size: 16px;
            color: #666;
        }

        /* Results */
        .results-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 30px;
            margin: 40px 0;
        }

        .result-item {
            text-align: center;
        }

        .result-item img {
            width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            margin-bottom: 15px;
        }

        .result-item h4 {
            font-size: 18px;
            font-weight: 600;
            color: #1a1a1a;
            margin-bottom: 10px;
        }

        .result-item p {
            font-size: 14px;
            color: #666;
        }

        /* Figures */
        .figure-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
            gap: 30px;
            margin: 40px 0;
        }

        .figure-item {
            background-color: #f8f9fa;
            border-radius: 8px;
            padding: 20px;
            box-shadow: 0 2px 12px rgba(0, 0, 0, 0.06);
        }

        .figure-item h4 {
            font-size: 18px;
            font-weight: 600;
            color: #1a1a1a;
            margin-bottom: 15px;
        }

        .figure-frame {
            width: 100%;
            height: 420px;
            border: none;
            border-radius: 6px;
            box-shadow: inset 0 0 0 1px rgba(0, 0, 0, 0.05);
            background-color: white;
        }

        .figure-caption {
            margin-top: 12px;
            font-size: 14px;
            color: #666;
            text-align: left;
        }

        /* Code Block */
        pre {
            background-color: #1a1a1a;
            color: #f8f9fa;
            padding: 24px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 30px 0;
            font-size: 14px;
            line-height: 1.6;
        }

        code {
            font-family: 'Monaco', 'Menlo', 'Ubuntu Mono', monospace;
        }

        /* Table */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 30px 0;
        }

        th, td {
            padding: 12px 16px;
            text-align: left;
            border-bottom: 1px solid #e0e0e0;
        }

        th {
            background-color: #f8f9fa;
            font-weight: 600;
            color: #1a1a1a;
        }

        tr:hover {
            background-color: #f8f9fa;
        }

        /* Lists */
        ul, ol {
            margin-left: 30px;
            margin-bottom: 20px;
        }

        li {
            font-size: 18px;
            color: #444;
            margin-bottom: 10px;
            line-height: 1.8;
        }

        /* Footer */
        footer {
            text-align: center;
            padding: 60px 0;
            margin-top: 80px;
            border-top: 1px solid #e0e0e0;
            color: #666;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .container {
                padding: 0 20px;
            }

            h1 {
                font-size: 36px;
            }

            h2 {
                font-size: 28px;
            }

            section {
                margin: 60px 0;
            }

            .abstract {
                padding: 24px;
            }

            .method-grid,
            .results-grid {
                grid-template-columns: 1fr;
            }

            .figure-grid {
                grid-template-columns: 1fr;
            }

            .figure-frame {
                height: 360px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>SuperF: Neural Implicit Fields for<br>Multi-Image Super-Resolution</h1>
            <p class="authors">Anonymous Authors</p>
            <div class="links">
                <a href="https://github.com/yourusername/superf" class="link-button">Code</a>
                <a href="../2025___ICLR___SuperF_neural_implicit_fields (3).pdf" class="link-button secondary">Paper</a>
            </div>
        </header>

        <div class="hero-image">
            <img src="../images/hr_image.png" alt="Satellite Super Resolution Results" onerror="this.style.display='none'">
        </div>

        <section>
            <h2>Abstract</h2>
            <div class="abstract">
                <p>
                    High-resolution imagery is often hindered by limitations in sensor technology, atmospheric 
                    conditions, and costs. Such challenges occur in satellite remote sensing, but also with handheld 
                    cameras, such as our smartphones. Hence, super-resolution aims to enhance the image resolution 
                    algorithmically. Since single-image super-resolution requires solving an inverse problem, such 
                    methods must exploit strong priors, e.g. learned from high-resolution training data, or be 
                    constrained by auxiliary data, e.g. by a high-resolution guide from another modality. While 
                    qualitatively pleasing, such approaches often lead to "hallucinated" structures that do not 
                    match reality. In contrast, multi-image super-resolution (MISR) aims to improve the (optical) 
                    resolution by constraining the super-resolution process with multiple views taken with sub-pixel 
                    shifts. Here, we propose SuperF, a test-time optimization approach for MISR that leverages 
                    coordinate-based neural networks, also called neural fields. Their ability to represent continuous 
                    signals with an implicit neural representation (INR) makes them an ideal fit for the MISR task. 
                    The key characteristic of our approach is to share an INR for multiple shifted low-resolution 
                    frames and to jointly optimize the frame alignment with the INR. Our approach advances related 
                    INR baselines, adopted from burst fusion for layer separation, by directly parameterizing the 
                    sub-pixel alignment as optimizable affine transformation parameters and by optimizing via a 
                    super-sampled coordinate grid that corresponds to the output resolution. Our experiments yield 
                    compelling results on simulated bursts of satellite imagery and ground-level images from handheld 
                    cameras, with upsampling factors of up to 8. A key advantage of SuperF is that this approach 
                    does not rely on any high-resolution training data.
                </p>
            </div>
        </section>

        <section>
            <h2>Method</h2>
            <p>
                SuperF achieves multi-image super-resolution by sharing an implicit neural representation (INR) 
                across multiple low-resolution (LR) frames with sub-pixel shifts. The LR frames are aligned by 
                jointly optimizing an affine coordinate transformation for each LR frame, together with the 
                parameters of a coordinate-based multi-layer perceptron (MLP) that decodes the input coordinates 
                to RGB values. Hence, leveraging the continuous characteristics of INRs for both the sub-pixel 
                alignment in the pixel coordinate space and for representing the underlying high-resolution (HR) signal.
            </p>

            <div class="method-grid">
                <div class="method-item">
                    <h4>1. Implicit Neural Representation</h4>
                    <p>
                        We use a coordinate-based MLP that maps continuous input coordinates (2D image locations) 
                        directly to RGB pixel intensities. This INR is shared across all frames.
                    </p>
                </div>
                <div class="method-item">
                    <h4>2. Joint Alignment Optimization</h4>
                    <p>
                        We directly parameterize affine transformations (translation and rotation) for each frame 
                        and jointly optimize them with the INR parameters, using the base frame as the reference 
                        coordinate system.
                    </p>
                </div>
                <div class="method-item">
                    <h4>3. Supersampling Strategy</h4>
                    <p>
                        We optimize the INR on a high-resolution coordinate grid that corresponds to the output 
                        resolution, then downsample for supervision with the LR frames, improving sub-pixel alignment.
                    </p>
                </div>
                <div class="method-item">
                    <h4>4. Fourier Features</h4>
                    <p>
                        We use Fourier feature positional encoding to overcome the spectral bias of coordinate-based 
                        MLPs and enable learning of high-frequency details.
                    </p>
                </div>
            </div>

            <h3>Key Contributions</h3>
            <ol>
                <li><strong>Test-time Optimization for MISR</strong> - We propose SuperF, a test-time optimization method 
                    for MISR based on implicit neural representations that jointly optimizes sub-pixel frame alignment 
                    with an MLP shared across frames.</li>
                <li><strong>Improved Sub-pixel Alignment</strong> - Our method yields improved alignment and continuous 
                    representation by directly parameterizing affine transformations and using a supersampling strategy.</li>
                <li><strong>No Training Data Required</strong> - As a TTO method, SuperF does not require any high-resolution 
                    training data, facilitating applicability to new domains and minimizing the risk of hallucinating 
                    high-resolution structures.</li>
            </ol>
        </section>

        <section>
            <h2>Results</h2>
            <p>
                We validate SuperF on two domains: satellite imagery (SatSynthBurst dataset) and ground-level 
                images from handheld cameras (SyntheticBurst dataset). Our approach outperforms existing test-time 
                optimization baselines including steerable kernel regression (Lafenetre et al., 2023) and NIR (Nam 
                et al., 2022), achieving upsampling factors of up to 8×.
            </p>

            <h3>Quantitative Results</h3>
            <p>
                On the SatSynthBurst dataset with upsampling factor ×4, SuperF achieves <strong>32.94 dB PSNR</strong>, 
                <strong>0.853 SSIM</strong>, and <strong>0.287 LPIPS</strong>, significantly outperforming bilinear upsampling 
                (29.71 dB PSNR) and other baselines. On the SyntheticBurst dataset, SuperF achieves 
                <strong>27.90 dB PSNR</strong>, <strong>0.774 SSIM</strong>, and <strong>0.385 LPIPS</strong> for ×4 upsampling.
            </p>

            <h3>Real-World Application</h3>
            <p>
                We demonstrate that SuperF can be applied to real-world satellite images from Sentinel-2, achieving 
                upsampling factors of 5× using filtered time series from Sentinel-2 STAC endpoints.
            </p>

            <div class="results-grid">
                <div class="result-item">
                    <img src="../images/hr_image.png" alt="Satellite Super-Resolution" onerror="this.style.display='none'">
                    <h4>Satellite Imagery</h4>
                    <p>Results on SatSynthBurst dataset</p>
                </div>
                <div class="result-item">
                    <img src="../images/hr_image.png" alt="Ground-Level Bursts" onerror="this.style.display='none'">
                    <h4>Ground-Level Bursts</h4>
                    <p>Results on SyntheticBurst dataset</p>
                </div>
                <div class="result-item">
                    <img src="../images/hr_image.png" alt="Real Sentinel-2" onerror="this.style.display='none'">
                    <h4>Real Sentinel-2</h4>
                    <p>Application to real satellite imagery</p>
                </div>
            </div>
        </section>

        <section>
            <h2>Figures</h2>
            <div class="figure-grid">
                <div class="figure-item">
                    <h4>Method Overview</h4>
                    <iframe class="figure-frame" src="figs/SuperF_method (1).pdf#view=FitH"></iframe>
                    <p class="figure-caption">
                        Illustration of the SuperF pipeline and the joint optimization of the implicit neural representation with frame alignment.
                        <a href="figs/SuperF_method (1).pdf">Download PDF</a>
                    </p>
                </div>
                <div class="figure-item">
                    <h4>Qualitative Comparisons</h4>
                    <iframe class="figure-frame" src="figs/qualitative_examples (2).pdf#view=FitH"></iframe>
                    <p class="figure-caption">
                        Qualitative results showcasing SuperF compared with existing methods across satellite and ground-level scenes.
                        <a href="figs/qualitative_examples (2).pdf">Download PDF</a>
                    </p>
                </div>
                <div class="figure-item">
                    <h4>Real-World Sentinel-2 Examples</h4>
                    <iframe class="figure-frame" src="figs/SuperF Real Ex-2.pdf#view=FitH"></iframe>
                    <p class="figure-caption">
                        Application of SuperF to real Sentinel-2 satellite imagery with upsampling factors of 5×.
                        <a href="figs/SuperF Real Ex-2.pdf">Download PDF</a>
                    </p>
                </div>
            </div>
        </section>

        <section>
            <h2>Quick Start</h2>
            <h3>Installation</h3>
            <pre><code>git clone https://github.com/yourusername/superf.git
cd superf
pip install -r requirements.txt</code></pre>

            <h3>Generate Training Data</h3>
            <pre><code>python create_data_from_single_image.py --input_image path/to/image.png --output_dir data/sample_1</code></pre>

            <h3>Training</h3>
            <pre><code>python main.py --dataset satburst_synth --sample_id sample_1 --df 4 --lr_shift 1.0 --iters 1000 --d 0</code></pre>
        </section>

        <section>
            <h2>Key Parameters</h2>
            <table>
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>Description</th>
                        <th>Default</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><code>--dataset</code></td>
                        <td>Dataset type: "satburst_synth", "worldstrat", "burst_synth"</td>
                        <td>"satburst_synth"</td>
                    </tr>
                    <tr>
                        <td><code>--df</code></td>
                        <td>Downsampling factor</td>
                        <td>4</td>
                    </tr>
                    <tr>
                        <td><code>--model</code></td>
                        <td>Model type: "mlp", "siren", "wire", "linear", "conv", "thera"</td>
                        <td>"mlp"</td>
                    </tr>
                    <tr>
                        <td><code>--input_projection</code></td>
                        <td>Input projection: "linear", "fourier_10", "legendre", "none"</td>
                        <td>"fourier_10"</td>
                    </tr>
                    <tr>
                        <td><code>--iters</code></td>
                        <td>Number of training iterations</td>
                        <td>1000</td>
                    </tr>
                    <tr>
                        <td><code>--lr_shift</code></td>
                        <td>Low-resolution shift amount</td>
                        <td>1.0</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <section>
            <h2>Citation</h2>
            <pre><code>@article{superf2026,
  title={SuperF: Neural Implicit Fields for Multi-Image Super-Resolution},
  author={Anonymous},
  journal={arXiv preprint},
  year={2026}
}</code></pre>
        </section>

        <footer>
            <p>&copy; 2026 SuperF Project</p>
            <p style="font-size: 14px; color: #999; margin-top: 10px;">
                Website template inspired by <a href="https://nerfies.github.io/" style="color: #666; text-decoration: none;">Nerfies</a>
            </p>
        </footer>
    </div>
</body>
</html>
